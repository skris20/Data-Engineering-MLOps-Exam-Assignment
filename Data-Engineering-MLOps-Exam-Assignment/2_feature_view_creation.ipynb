{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data & Feature views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import hopsworks\n",
    "import random\n",
    "import gdown\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data using pickle "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the DataFrame\n",
    "with open('ratebeer.pkl', 'rb') as file:\n",
    "    df = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connecting to Hopsworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connection closed.\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n",
      "\n",
      "Logged in to project, explore it here https://c.app.hopsworks.ai:443/p/550037\n",
      "Connected. Call `.close()` to terminate connection gracefully.\n"
     ]
    }
   ],
   "source": [
    "project = hopsworks.login()\n",
    "fs = project.get_feature_store()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"color:#ff5f27;\"> ⚙️ Feature View Creation </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<hsfs.feature_group.FeatureGroup object at 0x16f495d90>]\n",
      "[<hsfs.feature_group.FeatureGroup object at 0x16ded16a0>]\n",
      "[<hsfs.feature_group.FeatureGroup object at 0x16ec8e7b0>]\n",
      "[<hsfs.feature_group.FeatureGroup object at 0x127bbfb30>]\n",
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/550037/fs/545860/fv/user_beer_rating_feature_view/version/1\n",
      "Feature view created successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_user_beer_rating_feature_view(fs, version=1):\n",
    "    # Retrieve feature groups\n",
    "    beer_fg = fs.get_feature_group('beer_features', version=version)\n",
    "    review_fg = fs.get_feature_group('review_features', version=version)\n",
    "    agg_reviews_fg = fs.get_feature_group('agg_reviews', version=version)\n",
    "    reviewer_metrics_fg = fs.get_feature_group('reviewer_metrics', version=version)\n",
    "    \n",
    "    # Define the join queries properly\n",
    "    ds_query = beer_fg.select_all()\\\n",
    "        .join(review_fg.select_all(), on=['beer_beerid', 'review_profilename'])\\\n",
    "        .join(agg_reviews_fg.select_all(), on='beer_beerid')\\\n",
    "        .join(reviewer_metrics_fg.select_all(), on='review_profilename')\n",
    "    \n",
    "    # Define transformation functions for relevant features, exclude the label\n",
    "    transformation_functions = {\n",
    "        'review_aroma': fs.get_transformation_function(name='min_max_scaler'),\n",
    "        'review_taste': fs.get_transformation_function(name='min_max_scaler'),\n",
    "        'review_appearance': fs.get_transformation_function(name='min_max_scaler'),\n",
    "        'review_palate': fs.get_transformation_function(name='min_max_scaler'),\n",
    "    }\n",
    "    \n",
    "    # Create and return the feature view\n",
    "    return fs.create_feature_view(\n",
    "        name='user_beer_rating_feature_view',\n",
    "        version=version,\n",
    "        query=ds_query,\n",
    "        labels=['review_overall'],  # assuming 'review_overall' is your target variable\n",
    "        transformation_functions=transformation_functions,\n",
    "        description=\"Feature view aggregating user ratings with beer characteristics and review metrics\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    feature_view = create_user_beer_rating_feature_view(fs)\n",
    "    print(\"Feature view created successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to create feature view:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<hsfs.feature_group.FeatureGroup object at 0x16ded35f0>]\n",
      "[<hsfs.feature_group.FeatureGroup object at 0x16ece1bb0>]\n",
      "Feature view created successfully, explore it at \n",
      "https://c.app.hopsworks.ai:443/p/550037/fs/545860/fv/simple_feature_view/version/1\n",
      "Simple feature view created successfully.\n"
     ]
    }
   ],
   "source": [
    "def create_simple_feature_view(fs, version=1):\n",
    "    # Retrieve feature groups\n",
    "    beer_fg = fs.get_feature_group('beer_features', version=version)\n",
    "    review_fg = fs.get_feature_group('review_features', version=version)\n",
    "    \n",
    "    # Define the join query between beer_fg and review_fg\n",
    "    ds_query = beer_fg.select_all().join(review_fg.select_all(), on=['beer_beerid', 'review_profilename'])\n",
    "    \n",
    "    # Define transformation functions for relevant features, if needed\n",
    "    transformation_functions = {\n",
    "        'review_aroma': fs.get_transformation_function(name='min_max_scaler'),\n",
    "        'review_taste': fs.get_transformation_function(name='min_max_scaler'),\n",
    "        'review_appearance': fs.get_transformation_function(name='min_max_scaler'),\n",
    "        'review_palate': fs.get_transformation_function(name='min_max_scaler'),\n",
    "    }\n",
    "    \n",
    "    # Create and return the feature view\n",
    "    return fs.create_feature_view(\n",
    "        name='simple_feature_view',\n",
    "        version=version,\n",
    "        query=ds_query,\n",
    "        transformation_functions=transformation_functions,\n",
    "        description=\"Simple feature view with beer and review features\"\n",
    "    )\n",
    "\n",
    "try:\n",
    "    simple_feature_view = create_simple_feature_view(fs)\n",
    "    print(\"Simple feature view created successfully.\")\n",
    "except Exception as e:\n",
    "    print(\"Failed to create simple feature view:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot generate dataset(s) from the given start/end time because event time column is not available in the left feature groups. A start/end time should not be provided as parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_engine.py:165\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_batch_query\u001b[0;34m(self, feature_view_obj, start_time, end_time, with_label, primary_keys, event_time, inference_helper_columns, training_helper_columns, training_dataset_version, spine)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_event_time_to_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_event_time_to_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_python_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# verify whatever is passed 1. spine group with dataframe contained, or 2. dataframe\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# the schema has to be consistent\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# allow passing new spine group or dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_api.py:145\u001b[0m, in \u001b[0;36mFeatureViewApi.get_batch_query\u001b[0;34m(self, name, version, start_time, end_time, training_dataset_version, with_label, primary_keys, event_time, inference_helper_columns, training_helper_columns, is_python_engine)\u001b[0m\n\u001b[1;32m    137\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    138\u001b[0m     name,\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_VERSION,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_BATCH,\n\u001b[1;32m    143\u001b[0m ]\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query\u001b[38;5;241m.\u001b[39mQuery\u001b[38;5;241m.\u001b[39mfrom_response_json(\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_GET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_primary_keys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_event_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference_helper_columns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_helper_columns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_hive_engine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_python_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtd_version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/client/base.py:179\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/550037/featurestores/545860/featureview/simple_feature_view/version/1/query/batch). Server response: \nHTTP code: 400, HTTP reason: Bad Request, body: b'{\"errorCode\":270172,\"usrMsg\":\"Cannot find event feature in feature group beer_features\",\"errorMsg\":\"Event time feature not found\"}', error code: 270172, error msg: Event time feature not found, user msg: Cannot find event feature in feature group beer_features",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create training dataset with specified start and end dates\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m td_train_version, td_job_train \u001b[38;5;241m=\u001b[39m \u001b[43msimple_feature_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2000-04-12 00:00:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2009-12-30 00:00:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining dataset for user beer ratings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoalesce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwait_for_job\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create test dataset with specified start and end dates\u001b[39;00m\n\u001b[1;32m     12\u001b[0m td_test_version, td_job_test \u001b[38;5;241m=\u001b[39m simple_feature_view\u001b[38;5;241m.\u001b[39mcreate_training_data(\n\u001b[1;32m     13\u001b[0m     start_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2009-12-30 00:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     end_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2012-01-13 00:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     write_options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwait_for_job\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/usage.py:212\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    211\u001b[0m     exception \u001b[38;5;241m=\u001b[39m e\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/usage.py:208\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/feature_view.py:1403\u001b[0m, in \u001b[0;36mFeatureView.create_training_data\u001b[0;34m(self, start_time, end_time, storage_connector, location, description, extra_filter, data_format, coalesce, seed, statistics_config, write_options, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m   1386\u001b[0m td \u001b[38;5;241m=\u001b[39m training_dataset\u001b[38;5;241m.\u001b[39mTrainingDataset(\n\u001b[1;32m   1387\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1388\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     extra_filter\u001b[38;5;241m=\u001b[39mextra_filter,\n\u001b[1;32m   1401\u001b[0m )\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;66;03m# td_job is used only if the python engine is used\u001b[39;00m\n\u001b[0;32m-> 1403\u001b[0m td, td_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented version to `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(td\u001b[38;5;241m.\u001b[39mversion),\n\u001b[1;32m   1414\u001b[0m     util\u001b[38;5;241m.\u001b[39mVersionWarning,\n\u001b[1;32m   1415\u001b[0m )\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m td\u001b[38;5;241m.\u001b[39mversion, td_job\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_engine.py:268\u001b[0m, in \u001b[0;36mFeatureViewEngine.create_training_dataset\u001b[0;34m(self, feature_view_obj, training_dataset_obj, user_write_options, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_event_time(feature_view_obj, training_dataset_obj)\n\u001b[1;32m    265\u001b[0m updated_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_training_data_metadata(\n\u001b[1;32m    266\u001b[0m     feature_view_obj, training_dataset_obj\n\u001b[1;32m    267\u001b[0m )\n\u001b[0;32m--> 268\u001b[0m td_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_training_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updated_instance, td_job\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_engine.py:566\u001b[0m, in \u001b[0;36mFeatureViewEngine.compute_training_dataset\u001b[0;34m(self, feature_view_obj, user_write_options, training_dataset_obj, training_dataset_version, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo training dataset object or version is provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 566\u001b[0m batch_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_start_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_end_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# for spark job\u001b[39;00m\n\u001b[1;32m    580\u001b[0m user_write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_helper_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training_helper_columns\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_engine.py:202\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_batch_query\u001b[0;34m(self, feature_view_obj, start_time, end_time, with_label, primary_keys, event_time, inference_helper_columns, training_helper_columns, training_dataset_version, spine)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m270172\u001b[39m:\n\u001b[0;32m--> 202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot generate dataset(s) from the given start/end time because\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m event time column is not available in the left feature groups.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m A start/end time should not be provided as parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m         )\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot generate dataset(s) from the given start/end time because event time column is not available in the left feature groups. A start/end time should not be provided as parameters."
     ]
    }
   ],
   "source": [
    "# Create training dataset with specified start and end dates\n",
    "td_train_version, td_job_train = simple_feature_view.create_training_data(\n",
    "    start_time=\"2000-04-12 00:00:00\",\n",
    "    end_time=\"2009-12-30 00:00:00\",\n",
    "    description='Training dataset for user beer ratings',\n",
    "    data_format=\"csv\",\n",
    "    coalesce=True,\n",
    "    write_options={'wait_for_job': False},\n",
    ")\n",
    "\n",
    "# Create test dataset with specified start and end dates\n",
    "td_test_version, td_job_test = simple_feature_view.create_training_data(\n",
    "    start_time=\"2009-12-30 00:00:00\",\n",
    "    end_time=\"2012-01-13 00:00:00\",\n",
    "    description='Test dataset for user beer ratings',\n",
    "    data_format=\"csv\",\n",
    "    coalesce=True,\n",
    "    write_options={'wait_for_job': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_split_dates(df):\n",
    "    # Sort the DataFrame by the 'review_time' column\n",
    "    df_sorted = df.sort_values(by='review_time')\n",
    "    \n",
    "    # Calculate the index corresponding to 70% of the data\n",
    "    seventy_percent_index = int(0.7 * len(df_sorted))\n",
    "    \n",
    "    # Get the timestamp at the 70% index\n",
    "    seventy_percent_date = df_sorted.iloc[seventy_percent_index]['review_time']\n",
    "    \n",
    "    # Calculate the start and end dates for the training set\n",
    "    start_date_training = df_sorted.iloc[0]['review_time']\n",
    "    end_date_training = seventy_percent_date\n",
    "    \n",
    "    # Calculate the start and end dates for the test set\n",
    "    start_date_test = seventy_percent_date\n",
    "    end_date_test = df_sorted.iloc[-1]['review_time']\n",
    "    \n",
    "    return start_date_training, end_date_training, start_date_test, end_date_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DeprecationWarning: np.find_common_type is deprecated.  Please use `np.result_type` or `np.promote_types`.\n",
      "See https://numpy.org/devdocs/release/1.25.0-notes.html and the docs for more information.  (Deprecated NumPy 1.25)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Set Start Date: 2000-04-12 00:00:00\n",
      "Training Set End Date: 2009-12-30 00:00:00\n",
      "Test Set Start Date: 2009-12-30 00:00:00\n",
      "Test Set End Date: 2012-01-13 00:00:00\n"
     ]
    }
   ],
   "source": [
    "start_train, end_train, start_test, end_test = calculate_split_dates(df)\n",
    "print(\"Training Set Start Date:\", start_train)\n",
    "print(\"Training Set End Date:\", end_train)\n",
    "print(\"Test Set Start Date:\", start_test)\n",
    "print(\"Test Set End Date:\", end_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot generate dataset(s) from the given start/end time because event time column is not available in the left feature groups. A start/end time should not be provided as parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRestAPIError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_engine.py:165\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_batch_query\u001b[0;34m(self, feature_view_obj, start_time, end_time, with_label, primary_keys, event_time, inference_helper_columns, training_helper_columns, training_dataset_version, spine)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m        \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_event_time_to_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m        \u001b[49m\u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_event_time_to_timestamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_python_engine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpython\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwith_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m        \u001b[49m\u001b[43minference_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minference_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;66;03m# verify whatever is passed 1. spine group with dataframe contained, or 2. dataframe\u001b[39;00m\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# the schema has to be consistent\u001b[39;00m\n\u001b[1;32m    180\u001b[0m \n\u001b[1;32m    181\u001b[0m     \u001b[38;5;66;03m# allow passing new spine group or dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_api.py:145\u001b[0m, in \u001b[0;36mFeatureViewApi.get_batch_query\u001b[0;34m(self, name, version, start_time, end_time, training_dataset_version, with_label, primary_keys, event_time, inference_helper_columns, training_helper_columns, is_python_engine)\u001b[0m\n\u001b[1;32m    137\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_base_path \u001b[38;5;241m+\u001b[39m [\n\u001b[1;32m    138\u001b[0m     name,\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_VERSION,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_BATCH,\n\u001b[1;32m    143\u001b[0m ]\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query\u001b[38;5;241m.\u001b[39mQuery\u001b[38;5;241m.\u001b[39mfrom_response_json(\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_GET\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstart_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mend_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_label\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mwith_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_primary_keys\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwith_event_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minference_helper_columns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minference_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_helper_columns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_hive_engine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_python_engine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtd_version\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/decorators.py:35\u001b[0m, in \u001b[0;36mconnected.<locals>.if_connected\u001b[0;34m(inst, *args, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NoHopsworksConnectionError\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43minst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/client/base.py:179\u001b[0m, in \u001b[0;36mClient._send_request\u001b[0;34m(self, method, path_params, query_params, headers, data, stream, files)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError(url, response)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stream:\n",
      "\u001b[0;31mRestAPIError\u001b[0m: Metadata operation error: (url: https://c.app.hopsworks.ai/hopsworks-api/api/project/550037/featurestores/545860/featureview/user_beer_rating_feature_view/version/1/query/batch). Server response: \nHTTP code: 400, HTTP reason: Bad Request, body: b'{\"errorCode\":270172,\"usrMsg\":\"Cannot find event feature in feature group beer_features\",\"errorMsg\":\"Event time feature not found\"}', error code: 270172, error msg: Event time feature not found, user msg: Cannot find event feature in feature group beer_features",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create training dataset with specified start and end dates\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m td_train_version, td_job_train \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_view\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2000-04-12 00:00:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mend_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m2009-12-30 00:00:00\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTraining dataset for user beer ratings\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoalesce\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwait_for_job\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Create test dataset with specified start and end dates\u001b[39;00m\n\u001b[1;32m     12\u001b[0m td_test_version, td_job_test \u001b[38;5;241m=\u001b[39m feature_view\u001b[38;5;241m.\u001b[39mcreate_training_data(\n\u001b[1;32m     13\u001b[0m     start_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2009-12-30 00:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     14\u001b[0m     end_time\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2012-01-13 00:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     write_options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwait_for_job\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m},\n\u001b[1;32m     19\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/usage.py:212\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    211\u001b[0m     exception \u001b[38;5;241m=\u001b[39m e\n\u001b[0;32m--> 212\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/usage.py:208\u001b[0m, in \u001b[0;36mmethod_logger.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    205\u001b[0m exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# Call the original method\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/feature_view.py:1403\u001b[0m, in \u001b[0;36mFeatureView.create_training_data\u001b[0;34m(self, start_time, end_time, storage_connector, location, description, extra_filter, data_format, coalesce, seed, statistics_config, write_options, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m   1386\u001b[0m td \u001b[38;5;241m=\u001b[39m training_dataset\u001b[38;5;241m.\u001b[39mTrainingDataset(\n\u001b[1;32m   1387\u001b[0m     name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m   1388\u001b[0m     version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1400\u001b[0m     extra_filter\u001b[38;5;241m=\u001b[39mextra_filter,\n\u001b[1;32m   1401\u001b[0m )\n\u001b[1;32m   1402\u001b[0m \u001b[38;5;66;03m# td_job is used only if the python engine is used\u001b[39;00m\n\u001b[0;32m-> 1403\u001b[0m td, td_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_feature_view_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_training_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1404\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrite_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1409\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1411\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1412\u001b[0m warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1413\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented version to `\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(td\u001b[38;5;241m.\u001b[39mversion),\n\u001b[1;32m   1414\u001b[0m     util\u001b[38;5;241m.\u001b[39mVersionWarning,\n\u001b[1;32m   1415\u001b[0m )\n\u001b[1;32m   1417\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m td\u001b[38;5;241m.\u001b[39mversion, td_job\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_engine.py:268\u001b[0m, in \u001b[0;36mFeatureViewEngine.create_training_dataset\u001b[0;34m(self, feature_view_obj, training_dataset_obj, user_write_options, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_event_time(feature_view_obj, training_dataset_obj)\n\u001b[1;32m    265\u001b[0m updated_instance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_training_data_metadata(\n\u001b[1;32m    266\u001b[0m     feature_view_obj, training_dataset_obj\n\u001b[1;32m    267\u001b[0m )\n\u001b[0;32m--> 268\u001b[0m td_job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_training_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_write_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m updated_instance, td_job\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_engine.py:566\u001b[0m, in \u001b[0;36mFeatureViewEngine.compute_training_dataset\u001b[0;34m(self, feature_view_obj, user_write_options, training_dataset_obj, training_dataset_version, spine, primary_keys, event_time, training_helper_columns)\u001b[0m\n\u001b[1;32m    563\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo training dataset object or version is provided\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 566\u001b[0m batch_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_view_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_start_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevent_end_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwith_label\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprimary_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprimary_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevent_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevent_time\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43minference_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_helper_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining_dataset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_dataset_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    576\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mspine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;66;03m# for spark job\u001b[39;00m\n\u001b[1;32m    580\u001b[0m user_write_options[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_helper_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m training_helper_columns\n",
      "File \u001b[0;32m~/anaconda3/envs/MLOPS/lib/python3.12/site-packages/hsfs/core/feature_view_engine.py:202\u001b[0m, in \u001b[0;36mFeatureViewEngine.get_batch_query\u001b[0;34m(self, feature_view_obj, start_time, end_time, with_label, primary_keys, event_time, inference_helper_columns, training_helper_columns, training_dataset_version, spine)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mRestAPIError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mjson()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m270172\u001b[39m:\n\u001b[0;32m--> 202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    203\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot generate dataset(s) from the given start/end time because\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    204\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m event time column is not available in the left feature groups.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    205\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m A start/end time should not be provided as parameters.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    206\u001b[0m         )\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot generate dataset(s) from the given start/end time because event time column is not available in the left feature groups. A start/end time should not be provided as parameters."
     ]
    }
   ],
   "source": [
    "# Create training dataset with specified start and end dates\n",
    "td_train_version, td_job_train = feature_view.create_training_data(\n",
    "    start_time=\"2000-04-12 00:00:00\",\n",
    "    end_time=\"2009-12-30 00:00:00\",\n",
    "    description='Training dataset for user beer ratings',\n",
    "    data_format=\"csv\",\n",
    "    coalesce=True,\n",
    "    write_options={'wait_for_job': False},\n",
    ")\n",
    "\n",
    "# Create test dataset with specified start and end dates\n",
    "td_test_version, td_job_test = feature_view.create_training_data(\n",
    "    start_time=\"2009-12-30 00:00:00\",\n",
    "    end_time=\"2012-01-13 00:00:00\",\n",
    "    description='Test dataset for user beer ratings',\n",
    "    data_format=\"csv\",\n",
    "    coalesce=True,\n",
    "    write_options={'wait_for_job': False},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/550037/jobs/named/user_beer_rating_feature_view_1_create_fv_td_06052024165254/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `2`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset job started successfully, you can follow the progress at \n",
      "https://c.app.hopsworks.ai/p/550037/jobs/named/user_beer_rating_feature_view_1_create_fv_td_06052024165301/executions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VersionWarning: Incremented version to `3`.\n"
     ]
    }
   ],
   "source": [
    "# Create training dataset based on calculated start and end dates\n",
    "td_train_version, td_job_train = feature_view.create_training_data(\n",
    "    description='Training dataset for user beer ratings',\n",
    "    data_format=\"csv\",\n",
    "    coalesce=True,\n",
    "    write_options={'wait_for_job': False},\n",
    ")\n",
    "\n",
    "# Create test dataset based on calculated start and end dates\n",
    "td_test_version, td_job_test = feature_view.create_training_data(\n",
    "    description='Test dataset for user beer ratings',\n",
    "    data_format=\"csv\",\n",
    "    coalesce=True,\n",
    "    write_options={'wait_for_job': False},\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLOPS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
